{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False)), ('rf', RandomF...,\n  max_iter=-1, probability=False, random_state=None, shrinking=True,\n  tol=0.001, verbose=False))],\n         flatten_transform=None, n_jobs=1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = make_moons(n_samples=500, noise=0.3, random_state=42)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=42)\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard'\n",
    ")\n",
    "voting_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\nRandomForestClassifier 0.88\nVotingClassifier 0.896\nSVC 0.888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LeeSangHoon\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n  if diff:\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, voting_clf, svm_clf):\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_predict = clf.predict(x_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500, \n",
    "    max_samples=100, bootstrap=True, n_jobs=-1\n",
    ")\n",
    "bag_clf.fit(x_train, y_train)\n",
    "y_predict = bag_clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8986666666666666"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    bootstrap=True, n_jobs=-1, oob_score=True\n",
    ")\n",
    "\n",
    "bag_clf.fit(x_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = bag_clf.predict(x_test)\n",
    "accuracy_score(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.42045455, 0.57954545],\n       [0.37198068, 0.62801932],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [0.04477612, 0.95522388],\n       [0.38953488, 0.61046512],\n       [0.01714286, 0.98285714],\n       [1.        , 0.        ],\n       [0.97326203, 0.02673797],\n       [0.79144385, 0.20855615],\n       [0.00966184, 0.99033816],\n       [0.80446927, 0.19553073],\n       [0.86470588, 0.13529412],\n       [0.97382199, 0.02617801],\n       [0.06077348, 0.93922652],\n       [0.        , 1.        ],\n       [0.98333333, 0.01666667],\n       [0.95555556, 0.04444444],\n       [0.99435028, 0.00564972],\n       [0.01675978, 0.98324022],\n       [0.26903553, 0.73096447],\n       [0.91758242, 0.08241758],\n       [1.        , 0.        ],\n       [0.98067633, 0.01932367],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.62631579, 0.37368421],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [0.22222222, 0.77777778],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.3627451 , 0.6372549 ],\n       [0.00574713, 0.99425287],\n       [1.        , 0.        ],\n       [0.19653179, 0.80346821],\n       [0.40721649, 0.59278351],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [0.02688172, 0.97311828],\n       [1.        , 0.        ],\n       [0.00584795, 0.99415205],\n       [0.97126437, 0.02873563],\n       [0.86338798, 0.13661202],\n       [0.9754902 , 0.0245098 ],\n       [0.97282609, 0.02717391],\n       [0.        , 1.        ],\n       [0.05405405, 0.94594595],\n       [0.98265896, 0.01734104],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [0.00564972, 0.99435028],\n       [0.98757764, 0.01242236],\n       [0.78409091, 0.21590909],\n       [0.37362637, 0.62637363],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.6627907 , 0.3372093 ],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [0.84810127, 0.15189873],\n       [1.        , 0.        ],\n       [0.64893617, 0.35106383],\n       [0.13089005, 0.86910995],\n       [0.63265306, 0.36734694],\n       [0.88764045, 0.11235955],\n       [0.        , 1.        ],\n       [0.24022346, 0.75977654],\n       [0.90673575, 0.09326425],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.05882353, 0.94117647],\n       [0.03296703, 0.96703297],\n       [0.32758621, 0.67241379],\n       [1.        , 0.        ],\n       [0.00537634, 0.99462366],\n       [0.85427136, 0.14572864],\n       [0.01530612, 0.98469388],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [0.20224719, 0.79775281],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [0.94797688, 0.05202312],\n       [0.78191489, 0.21808511],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.22772277, 0.77227723],\n       [0.68292683, 0.31707317],\n       [0.        , 1.        ],\n       [0.03314917, 0.96685083],\n       [0.50943396, 0.49056604],\n       [1.        , 0.        ],\n       [0.02717391, 0.97282609],\n       [1.        , 0.        ],\n       [0.19371728, 0.80628272],\n       [0.43523316, 0.56476684],\n       [1.        , 0.        ],\n       [0.00518135, 0.99481865],\n       [0.98941799, 0.01058201],\n       [0.23428571, 0.76571429],\n       [0.90217391, 0.09782609],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [0.83516484, 0.16483516],\n       [1.        , 0.        ],\n       [0.00534759, 0.99465241],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [0.99459459, 0.00540541],\n       [1.        , 0.        ],\n       [0.00546448, 0.99453552],\n       [0.95505618, 0.04494382],\n       [1.        , 0.        ],\n       [0.01648352, 0.98351648],\n       [0.24479167, 0.75520833],\n       [0.95767196, 0.04232804],\n       [0.25257732, 0.74742268],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [0.76216216, 0.23783784],\n       [0.40625   , 0.59375   ],\n       [0.48663102, 0.51336898],\n       [0.87349398, 0.12650602],\n       [0.93370166, 0.06629834],\n       [0.05102041, 0.94897959],\n       [0.76047904, 0.23952096],\n       [0.00568182, 0.99431818],\n       [0.        , 1.        ],\n       [0.01098901, 0.98901099],\n       [0.97894737, 0.02105263],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [0.01595745, 0.98404255],\n       [0.        , 1.        ],\n       [0.00947867, 0.99052133],\n       [0.00505051, 0.99494949],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [0.94565217, 0.05434783],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.40112994, 0.59887006],\n       [0.2565445 , 0.7434555 ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [0.27659574, 0.72340426],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.00526316, 0.99473684],\n       [0.        , 1.        ],\n       [0.98901099, 0.01098901],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.00549451, 0.99450549],\n       [0.62686567, 0.37313433],\n       [0.90960452, 0.09039548],\n       [0.        , 1.        ],\n       [0.98314607, 0.01685393],\n       [0.99459459, 0.00540541],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.07253886, 0.92746114],\n       [1.        , 0.        ],\n       [0.03825137, 0.96174863],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.03888889, 0.96111111],\n       [0.99435028, 0.00564972],\n       [0.945     , 0.055     ],\n       [0.76111111, 0.23888889],\n       [0.5819209 , 0.4180791 ],\n       [0.        , 1.        ],\n       [0.15789474, 0.84210526],\n       [1.        , 0.        ],\n       [0.95360825, 0.04639175],\n       [0.9787234 , 0.0212766 ],\n       [1.        , 0.        ],\n       [0.01142857, 0.98857143],\n       [0.        , 1.        ],\n       [0.40331492, 0.59668508],\n       [0.87931034, 0.12068966],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.02209945, 0.97790055],\n       [0.        , 1.        ],\n       [0.95833333, 0.04166667],\n       [0.        , 1.        ],\n       [0.3038674 , 0.6961326 ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [0.97647059, 0.02352941],\n       [0.82010582, 0.17989418],\n       [1.        , 0.        ],\n       [0.00571429, 0.99428571],\n       [0.07608696, 0.92391304],\n       [1.        , 0.        ],\n       [0.04571429, 0.95428571],\n       [0.        , 1.        ],\n       [0.0441989 , 0.9558011 ],\n       [1.        , 0.        ],\n       [0.75438596, 0.24561404],\n       [0.        , 1.        ],\n       [0.90810811, 0.09189189],\n       [0.98469388, 0.01530612],\n       [0.21348315, 0.78651685],\n       [0.25414365, 0.74585635],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [0.24157303, 0.75842697],\n       [0.95897436, 0.04102564],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.9950495 , 0.0049505 ],\n       [0.        , 1.        ],\n       [0.52      , 0.48      ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [0.06432749, 0.93567251],\n       [0.13114754, 0.86885246],\n       [0.97687861, 0.02312139],\n       [0.01092896, 0.98907104],\n       [1.        , 0.        ],\n       [0.42622951, 0.57377049],\n       [0.08379888, 0.91620112],\n       [0.52486188, 0.47513812],\n       [0.66483516, 0.33516484],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [0.63829787, 0.36170213],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.22105263, 0.77894737],\n       [0.77222222, 0.22777778],\n       [0.04705882, 0.95294118],\n       [1.        , 0.        ],\n       [0.79518072, 0.20481928],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [0.13095238, 0.86904762],\n       [0.02185792, 0.97814208],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.90532544, 0.09467456],\n       [0.15263158, 0.84736842],\n       [0.95454545, 0.04545455],\n       [0.00546448, 0.99453552],\n       [0.59302326, 0.40697674],\n       [0.05681818, 0.94318182],\n       [0.99009901, 0.00990099],\n       [0.87790698, 0.12209302],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.96774194, 0.03225806],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.31578947, 0.68421053],\n       [0.99438202, 0.00561798],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.00526316, 0.99473684],\n       [0.88586957, 0.11413043],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.72192513, 0.27807487],\n       [0.97191011, 0.02808989],\n       [1.        , 0.        ],\n       [0.77222222, 0.22777778],\n       [0.5359116 , 0.4640884 ],\n       [0.        , 1.        ],\n       [0.90909091, 0.09090909],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.84117647, 0.15882353],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [0.80208333, 0.19791667],\n       [0.12980769, 0.87019231],\n       [0.42458101, 0.57541899],\n       [0.21022727, 0.78977273],\n       [0.        , 1.        ],\n       [0.88709677, 0.11290323],\n       [0.84431138, 0.15568862],\n       [0.00552486, 0.99447514],\n       [1.        , 0.        ],\n       [0.98802395, 0.01197605],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.01913876, 0.98086124],\n       [0.97126437, 0.02873563],\n       [0.96022727, 0.03977273],\n       [1.        , 0.        ],\n       [0.5106383 , 0.4893617 ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.99516908, 0.00483092],\n       [0.00581395, 0.99418605],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.96446701, 0.03553299],\n       [0.        , 1.        ],\n       [0.11299435, 0.88700565],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.01970443, 0.98029557],\n       [1.        , 0.        ],\n       [0.13559322, 0.86440678],\n       [0.        , 1.        ],\n       [0.00588235, 0.99411765],\n       [0.        , 1.        ],\n       [0.41326531, 0.58673469],\n       [0.07329843, 0.92670157],\n       [0.22335025, 0.77664975],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [0.14606742, 0.85393258],\n       [0.98369565, 0.01630435],\n       [0.01030928, 0.98969072],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.95054945, 0.04945055],\n       [0.3       , 0.7       ],\n       [0.99378882, 0.00621118],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.98275862, 0.01724138],\n       [0.        , 1.        ],\n       [0.03208556, 0.96791444],\n       [0.98378378, 0.01621622],\n       [1.        , 0.        ],\n       [0.02688172, 0.97311828],\n       [0.67015707, 0.32984293]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(x_train, y_train)\n",
    "\n",
    "y_predict = rnd_clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),\n",
    "    n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.10004759283269912\nsepal width (cm) 0.024426966019725715\npetal length (cm) 0.4194658331613095\npetal width (cm) 0.4560596079862658\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n            max_features=None, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n            splitter='best'),\n          learning_rate=0.5, n_estimators=200, random_state=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=200, \n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5)\n",
    "ada_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n           max_leaf_nodes=None, min_impurity_decrease=0.0,\n           min_impurity_split=None, min_samples_leaf=1,\n           min_samples_split=2, min_weight_fraction_leaf=0.0,\n           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n           max_leaf_nodes=None, min_impurity_decrease=0.0,\n           min_impurity_split=None, min_samples_leaf=1,\n           min_samples_split=2, min_weight_fraction_leaf=0.0,\n           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = Y -tree_reg1.predict(X) \n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=3, max_features=None,\n           max_leaf_nodes=None, min_impurity_decrease=0.0,\n           min_impurity_split=None, min_samples_leaf=1,\n           min_samples_split=2, min_weight_fraction_leaf=0.0,\n           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=3)\n",
    "tree_reg3.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sum(tree.predict(X) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n             learning_rate=1.0, loss='ls', max_depth=2, max_features=None,\n             max_leaf_nodes=None, min_impurity_decrease=0.0,\n             min_impurity_split=None, min_samples_leaf=1,\n             min_samples_split=2, min_weight_fraction_leaf=0.0,\n             n_estimators=3, presort='auto', random_state=None,\n             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
    "gbrt.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0 \n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y)\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(x_train, y_train)\n",
    "    y_pred = gbrt.predict(x_test)\n",
    "    val_error = mean_squared_error(y_pred, y_test)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        if error_going_up == 5:\n",
    "            break\n",
    "        error_going_up+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
